{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044dbdcf",
   "metadata": {},
   "source": [
    "## Part One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b83d47",
   "metadata": {},
   "source": [
    "### Load Pickled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122909a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(Path.cwd() / \"pickles\" /\"parsed.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25442120",
   "metadata": {},
   "source": [
    "### Scratched/Tests scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b7b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get column names\n",
    "df.columns\n",
    "# initialise dictionary\n",
    "adj_dict = {}\n",
    "# Loops thru each row in the df and gets the spacy doc\n",
    "for index, row in df.iterrows():\n",
    "    print(\"-\"*30)\n",
    "    print(row['title'])\n",
    "    doc = row['parsed']\n",
    "    # Loop thru each word of each spacy doc\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            adj = token.lemma_\n",
    "            adj_dict[adj] = adj_dict.get(adj, 0) + 1\n",
    "\n",
    "# Convert dictionary to lsit of tuples\n",
    "adj_list = list(adj_dict.items())\n",
    "\n",
    "\n",
    "print(adj_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c1037",
   "metadata": {},
   "source": [
    "#### Test subject by verb count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d52e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test subject_by_verb_count\n",
    "import importlib\n",
    "import PartOne as po\n",
    "\n",
    "importlib.reload(po)\n",
    "\n",
    "\n",
    "df_mini = df.iloc[[0]]\n",
    "#print(df_mini)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    print(row['title'])\n",
    "    for pair in po.subjects_by_verb_count(row['parsed'], 'run'):\n",
    "        print(f\"\\t {pair}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207afd9",
   "metadata": {},
   "source": [
    "### Developing Pointwise mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1856ee",
   "metadata": {},
   "source": [
    "Use only 1 row of the data frame for development only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d29737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title                                            dummy_text\n",
      "text      The cat hears a mouse. The dog hears a noise. ...\n",
      "parsed    (The, cat, hears, a, mouse, ., The, dog, hears...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "df_mini = df.iloc[0]\n",
    "\n",
    "data = [{'title': \"dummy_text\", 'text': 'The cat hears a mouse. The dog hears a noise. The cat hears the dog.'}]\n",
    "\n",
    "df_mini = pd.DataFrame(data)\n",
    "\n",
    "# load your model\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add parse\n",
    "\n",
    "df_mini['parsed'] = df_mini['text'].apply(nlp)\n",
    "df_mini = df_mini.iloc[0]\n",
    "\n",
    "print(df_mini.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb963a1",
   "metadata": {},
   "source": [
    "#### Create the Subject-Verb pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "67605fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_text\n",
      "1. Counts of Verbs and Subjects:\n",
      "\t[{('hear', 'cat'): 2}, {('hear', 'dog'): 1}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count of co occurrence\n",
    "verb = 'hear'\n",
    "verb_subject = po.subjects_by_verb_count(df_mini['parsed'], verb)\n",
    "\n",
    "# for doc in df_mini['parsed']:\n",
    "#     verb_subject = (po.subjects_by_verb_count(doc, \"hear\"))\n",
    "print(df_mini['title'])\n",
    "print(f\"1. Counts of Verbs and Subjects:\\n\\t{verb_subject}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa6e34",
   "metadata": {},
   "source": [
    "#### Extract unique words from the S-V pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5509b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Extract unique words for total counts: \n",
      "\t{'cat', 'hear', 'dog'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract unique objects\n",
    "unique_w = set()\n",
    "for d in verb_subject:\n",
    "    for k in d.keys():\n",
    "        for ind in k:\n",
    "            unique_w.add(ind)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"2. Extract unique words for total counts: \\n\\t{unique_w}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d871811",
   "metadata": {},
   "source": [
    "#### Count total words, and count of individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b1cfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "cat\n",
      "hear\n",
      "a\n",
      "mouse\n",
      "the\n",
      "dog\n",
      "hear\n",
      "a\n",
      "noise\n",
      "the\n",
      "cat\n",
      "hear\n",
      "the\n",
      "dog\n",
      "3. Total words and total count for existing words: \n",
      "\tTotal words: 15\n",
      "\tTotal count for existing:\n",
      "\t{'cat': 2, 'hear': 3, 'dog': 2}\n"
     ]
    }
   ],
   "source": [
    "# Count word occurrence in whole document, and total tokens in doc\n",
    "total_words = 0\n",
    "# Create and add keys to count the expected words\n",
    "total_existing = {w: 0 for w in unique_w}\n",
    "\n",
    "for token in df_mini['parsed']:\n",
    "\n",
    "    if token.text.isalpha():\n",
    "        total_words += 1\n",
    "        word_lemma = token.lemma_\n",
    "        if word_lemma in total_existing:\n",
    "            total_existing[word_lemma] += 1\n",
    "print(f\"3. Total words and total count for existing words: \\n\\tTotal words: {total_words}\\n\\tTotal count for existing:\\n\\t{total_existing}\")\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e36d18",
   "metadata": {},
   "source": [
    "#### Calculate PPMI for the pair of the top 10 S-V pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "005fa7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: ('hear', 'cat'), PPMI: 2.322\n",
      "Key: ('hear', 'dog'), PPMI: 1.322\n",
      "[(('hear', 'cat'), 2.322), (('hear', 'dog'), 1.322)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Probability of the verb (context)\n",
    "p_c = total_existing[verb]/total_words\n",
    "ppmi_dict = {}\n",
    "# Loop over the dictionary of the 10 s-v pairs\n",
    "for d in verb_subject:\n",
    "    for key, value in d.items():\n",
    "        # probability of the pair verb-subject\n",
    "        p_wc = value/total_words\n",
    "        # Probability of the subject (word)\n",
    "        p_w = total_existing[key[1]]/total_words\n",
    "        # Calculate PMI\n",
    "        pmi = p_wc/(p_w * p_c)\n",
    "        pmi = math.log2(pmi)\n",
    "        ppmi = max(pmi,0)\n",
    "        print(f\"Key: {key}, PPMI: {round(ppmi,3)}\")\n",
    "        # Add value to final dict\n",
    "        ppmi_dict[key] = ppmi_dict.get(key, round(ppmi,3))\n",
    "# Sort final dictionary\n",
    "ppmi_dict = sorted(ppmi_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "\n",
    "print(ppmi_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d366b64",
   "metadata": {},
   "source": [
    "### Counting all syntactic objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047769c",
   "metadata": {},
   "source": [
    "Understanding dep_ tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecb5dffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(nlp.pipe_labels['parser'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3516ce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "{'nsubjpass', 'nmod', 'compound', 'prep', 'oprd', 'dative', 'conj', 'attr', 'preconj', 'mark', 'relcl', 'ccomp', 'nsubj', 'det', 'appos', 'auxpass', 'pcomp', 'dep', 'parataxis', 'advmod', 'quantmod', 'advcl', 'neg', 'acl', 'punct', 'prt', 'intj', 'dobj', 'case', 'predet', 'acomp', 'pobj', 'npadvmod', 'xcomp', 'ROOT', 'csubj', 'aux', 'cc', 'csubjpass', 'agent', 'meta', 'nummod', 'expl', 'amod', 'poss'}\n"
     ]
    }
   ],
   "source": [
    "synt_obj = set()\n",
    "\n",
    "for doc in df['parsed']:\n",
    "    for token in doc:\n",
    "        synt_obj.add(token.dep_)\n",
    "\n",
    "print('-----------')\n",
    "print(synt_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa1a0b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "nsubjpass: nominal subject (passive)\n",
      "--------------------\n",
      "--------------------\n",
      "nmod: modifier of nominal\n",
      "--------------------\n",
      "--------------------\n",
      "compound: compound\n",
      "--------------------\n",
      "--------------------\n",
      "prep: prepositional modifier\n",
      "--------------------\n",
      "--------------------\n",
      "oprd: object predicate\n",
      "--------------------\n",
      "--------------------\n",
      "dative: dative\n",
      "--------------------\n",
      "--------------------\n",
      "conj: conjunct\n",
      "--------------------\n",
      "--------------------\n",
      "attr: attribute\n",
      "--------------------\n",
      "--------------------\n",
      "preconj: pre-correlative conjunction\n",
      "--------------------\n",
      "--------------------\n",
      "mark: marker\n",
      "--------------------\n",
      "--------------------\n",
      "relcl: relative clause modifier\n",
      "--------------------\n",
      "--------------------\n",
      "ccomp: clausal complement\n",
      "--------------------\n",
      "--------------------\n",
      "nsubj: nominal subject\n",
      "--------------------\n",
      "--------------------\n",
      "det: determiner\n",
      "--------------------\n",
      "--------------------\n",
      "appos: appositional modifier\n",
      "--------------------\n",
      "--------------------\n",
      "auxpass: auxiliary (passive)\n",
      "--------------------\n",
      "--------------------\n",
      "pcomp: complement of preposition\n",
      "--------------------\n",
      "--------------------\n",
      "dep: unclassified dependent\n",
      "--------------------\n",
      "--------------------\n",
      "parataxis: parataxis\n",
      "--------------------\n",
      "--------------------\n",
      "advmod: adverbial modifier\n",
      "--------------------\n",
      "--------------------\n",
      "quantmod: modifier of quantifier\n",
      "--------------------\n",
      "--------------------\n",
      "advcl: adverbial clause modifier\n",
      "--------------------\n",
      "--------------------\n",
      "neg: negation modifier\n",
      "--------------------\n",
      "--------------------\n",
      "acl: clausal modifier of noun (adjectival clause)\n",
      "--------------------\n",
      "--------------------\n",
      "punct: punctuation\n",
      "--------------------\n",
      "--------------------\n",
      "prt: particle\n",
      "--------------------\n",
      "--------------------\n",
      "intj: interjection\n",
      "--------------------\n",
      "**************************************************\n",
      "dobj: direct object\n",
      "**************************************************\n",
      "--------------------\n",
      "case: case marking\n",
      "--------------------\n",
      "--------------------\n",
      "predet: None\n",
      "--------------------\n",
      "--------------------\n",
      "acomp: adjectival complement\n",
      "--------------------\n",
      "**************************************************\n",
      "pobj: object of preposition\n",
      "**************************************************\n",
      "--------------------\n",
      "npadvmod: noun phrase as adverbial modifier\n",
      "--------------------\n",
      "--------------------\n",
      "xcomp: open clausal complement\n",
      "--------------------\n",
      "--------------------\n",
      "ROOT: root\n",
      "--------------------\n",
      "--------------------\n",
      "csubj: clausal subject\n",
      "--------------------\n",
      "--------------------\n",
      "aux: auxiliary\n",
      "--------------------\n",
      "--------------------\n",
      "cc: coordinating conjunction\n",
      "--------------------\n",
      "--------------------\n",
      "csubjpass: clausal subject (passive)\n",
      "--------------------\n",
      "--------------------\n",
      "agent: agent\n",
      "--------------------\n",
      "--------------------\n",
      "meta: meta modifier\n",
      "--------------------\n",
      "--------------------\n",
      "nummod: numeric modifier\n",
      "--------------------\n",
      "--------------------\n",
      "expl: expletive\n",
      "--------------------\n",
      "--------------------\n",
      "amod: adjectival modifier\n",
      "--------------------\n",
      "--------------------\n",
      "poss: possession modifier\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for t in synt_obj:\n",
    "\n",
    "    exp = spacy.explain(t)\n",
    "\n",
    "    #if exp is not None and \"object\" in exp:\n",
    "    if 'obj' in t:\n",
    "        print(\"*\"*50)\n",
    "        print(f\"{t}: {exp}\")\n",
    "        print(\"*\"*50)\n",
    "    else:\n",
    "        print(\"-\"*20)\n",
    "        print(f\"{t}: {exp}\")\n",
    "        print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3140c4",
   "metadata": {},
   "source": [
    "Counting all syntactic objects loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "syntatic_object = {}\n",
    "none_count = {}\n",
    "for doc in df['parsed']:\n",
    "    for token in doc:\n",
    "        # Extract the type of dependency, explained\n",
    "        dep_explained = spacy.explain(token.dep_)\n",
    "        if dep_explained is None:\n",
    "            word_lemma = token.lemma_\n",
    "            none_count[word_lemma] = none_count.get(word_lemma, 0) + 1\n",
    "        elif \"object\" in dep_explained:\n",
    "            # Lemmatize word\n",
    "            word_lemma = token.lemma_\n",
    "            syntatic_object[word_lemma] = syntatic_object.get(word_lemma, 0) + 1\n",
    "\n",
    "print(f\"========= With Dependencies =========\")\n",
    "for key, value in syntatic_object.items():\n",
    "    print(f\"Key {key} Value {value}\")\n",
    "\n",
    "print(f\"\\n========= Without Dependencies (None) =========\")\n",
    "for key, value in none_count.items():\n",
    "    print(f\"Key {key} Value {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eaa031",
   "metadata": {},
   "source": [
    "Test that logic works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1383cc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/enmanuelmoreno/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mouse': 2, 'cat': 1, 'it': 1, 'garden': 1}\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import PartOne as po\n",
    "import spacy\n",
    "importlib.reload(po)\n",
    "\n",
    "text = 'the cat chased the mouse and caught the mouse, but the dog chased the cat and found it in the garden.'\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text_nlp = nlp(text)\n",
    "object_tags = ['dobj', 'iobj', 'oprd', 'obj', 'pobj']\n",
    "syntatic_object = {}\n",
    "\n",
    "for token in text_nlp:\n",
    "    # Extract the type of dependency, explained\n",
    "    dep_tag = token.dep_\n",
    "    if dep_tag in object_tags:\n",
    "        # Lemmatize word\n",
    "        word_lemma = token.lemma_\n",
    "        syntatic_object[word_lemma] = syntatic_object.get(word_lemma, 0) + 1\n",
    "\n",
    "print(syntatic_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70234406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/enmanuelmoreno/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/spacy/glossary.py:20: UserWarning: [W118] Term 'predet' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense_and_Sensibility\n",
      "[{'it': 706}, {'she': 686}, {'he': 517}, {'they': 405}, {'I': 340}, {'you': 330}, {'which': 251}, {'what': 210}, {'time': 204}, {'herself': 192}]\n",
      "North_and_South\n",
      "[{'she': 884}, {'it': 878}, {'he': 815}, {'I': 587}, {'you': 443}, {'which': 415}, {'what': 389}, {'they': 370}, {'time': 319}, {'Margaret': 242}]\n",
      "A_Tale_of_Two_Cities\n",
      "[{'he': 898}, {'it': 859}, {'you': 436}, {'I': 430}, {'they': 371}, {'she': 349}, {'hand': 295}, {'which': 222}, {'time': 201}, {'himself': 186}]\n",
      "Erewhon\n",
      "[{'which': 415}, {'I': 373}, {'it': 358}, {'they': 307}, {'he': 185}, {'time': 135}, {'that': 121}, {'one': 109}, {'what': 108}, {'we': 101}]\n",
      "The_American\n",
      "[{'it': 876}, {'you': 721}, {'he': 712}, {'I': 657}, {'she': 565}, {'what': 326}, {'that': 289}, {'they': 275}, {'Newman': 237}, {'hand': 232}]\n",
      "Dorian_Gray\n",
      "[{'he': 607}, {'it': 472}, {'I': 439}, {'you': 356}, {'that': 261}, {'life': 193}, {'they': 187}, {'what': 180}, {'she': 178}, {'thing': 131}]\n",
      "Tess_of_the_DUrbervilles\n",
      "[{'she': 862}, {'it': 624}, {'he': 577}, {'I': 464}, {'you': 394}, {'they': 374}, {'which': 373}, {'time': 235}, {'that': 215}, {'what': 200}]\n",
      "The_Golden_Bowl\n",
      "[{'it': 1793}, {'she': 1498}, {'he': 1218}, {'which': 763}, {'what': 745}, {'they': 678}, {'I': 487}, {'you': 466}, {'that': 409}, {'time': 358}]\n",
      "The_Secret_Garden\n",
      "[{'it': 626}, {'he': 449}, {'she': 351}, {'I': 211}, {'they': 207}, {'garden': 185}, {'thing': 163}, {'what': 159}, {'you': 141}, {'one': 114}]\n",
      "Portrait_of_the_Artist\n",
      "[{'he': 610}, {'it': 304}, {'they': 217}, {'which': 168}, {'you': 157}, {'eye': 125}, {'hand': 120}, {'what': 109}, {'God': 109}, {'I': 109}]\n",
      "The_Black_Moth\n",
      "[{'he': 738}, {'you': 522}, {'I': 489}, {'it': 470}, {'she': 416}, {'hand': 217}, {'that': 168}, {'eye': 142}, {'_': 140}, {'what': 133}]\n",
      "Orlando\n",
      "[{'it': 313}, {'she': 279}, {'he': 255}, {'they': 217}, {'which': 195}, {'time': 103}, {'that': 99}, {'hand': 98}, {'man': 89}, {'life': 80}]\n",
      "Blood_Meridian\n",
      "[{'they': 665}, {'he': 659}, {'it': 576}, {'horse': 285}, {'man': 269}, {'hand': 221}, {'head': 185}, {'fire': 161}, {'one': 127}, {'what': 124}]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import PartOne as po\n",
    "\n",
    "importlib.reload(po)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    print(row['title'])\n",
    "    print(po.count_obj(row['parsed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb03cec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/enmanuelmoreno/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'mouse': 2}, {'cat': 1}, {'it': 1}, {'garden': 1}]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import PartOne as po\n",
    "import spacy\n",
    "importlib.reload(po)\n",
    "\n",
    "text = 'the cat chased the mouse and caught the mouse, but the dog chased the cat and found it in the garden.'\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text_nlp = nlp(text)\n",
    "\n",
    "count_dict = po.count_obj(text_nlp)\n",
    "\n",
    "print(count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4244a5e9",
   "metadata": {},
   "source": [
    "## Part Two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5aff10",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c30897a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>party</th>\n",
       "      <th>constituency</th>\n",
       "      <th>date</th>\n",
       "      <th>speech_class</th>\n",
       "      <th>major_heading</th>\n",
       "      <th>year</th>\n",
       "      <th>speakername</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unemployment is soaring, uptake in benefits ha...</td>\n",
       "      <td>Labour</td>\n",
       "      <td>Portsmouth South</td>\n",
       "      <td>2020-09-14</td>\n",
       "      <td>Speech</td>\n",
       "      <td>Work and Pensions</td>\n",
       "      <td>2020</td>\n",
       "      <td>Stephen Morgan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I thank the hon. Gentleman for raising issues ...</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>Mid Sussex</td>\n",
       "      <td>2020-09-14</td>\n",
       "      <td>Speech</td>\n",
       "      <td>Work and Pensions</td>\n",
       "      <td>2020</td>\n",
       "      <td>Mims Davies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              speech         party  \\\n",
       "0  Unemployment is soaring, uptake in benefits ha...        Labour   \n",
       "1  I thank the hon. Gentleman for raising issues ...  Conservative   \n",
       "\n",
       "       constituency        date speech_class      major_heading  year  \\\n",
       "0  Portsmouth South  2020-09-14       Speech  Work and Pensions  2020   \n",
       "1        Mid Sussex  2020-09-14       Speech  Work and Pensions  2020   \n",
       "\n",
       "      speakername  \n",
       "0  Stephen Morgan  \n",
       "1     Mims Davies  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PartTwo as pt\n",
    "import importlib\n",
    "importlib.reload(pt)\n",
    "\n",
    "speeches_df = pt.read_csv()\n",
    "speeches_df.head(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
