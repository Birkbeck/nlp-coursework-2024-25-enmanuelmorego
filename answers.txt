Answers to the text questions go here.

Part 1
Question D

The Felsch Kincaid Grade level score does not take into account domain-specific terminology. For example, a text could have a good FK-score but the words, whilst short, 
can be very specific to a given domain, making it difficult to read for readers not familiar with such content. For example, words like 'dyspnea' or 'heloma' are not longer than 
other common words like 'breakfast' or 'computer', but sentences with words 'breakfast' and 'computer' might be easier to read than 'dyspnea' or 'heloma' 
given that 'computer' / 'breakfast' are more commonly used in everyday language. Therefore, FK Grade level does not directly take into account vocabulary difficulty.

Another case where the FK-Grade level would not be suitable is for very short or long texts. The main information that this metric uses is counts of words, sentences and syllables. If a 
text is too short, there won't be enough data for the formula to accurately represent the reading difficulty level of the text. On the other hand, if a document is too
long, it might be more appropriate to find the natural split of the corpus (is it organised by chapters, themes, sections), as these subcomponents of the document might
have their difficulty reading level. Thus, separating it into smaller subtexts might provide a more accurate representation of the reading difficulty. 

Part 2
Question E

Various tokenizers with different functionality were tested and after evaluation, my_tokenizer_contractions_clean() provided the best balance between performance and efficiency.

The tokenizer uses the pre-trained SpaCy model 'en_core_web_sm'. This contains a large list of English words along with their information (is the word a noun, verb, etc). It also 
contains specific tokenization rules, parsing, part of speech tagging etc. All of this is available in the nlp object that is created. 

The nlp object is created which holds all of the information and processing pipelines stored in 'en_core_web_sm'.

Then a custom tokenier is created using the SpaCy Tokenizer and the rules in the 'en_core_wb_sm' model. This tokenizer object is used later to tokenize the text. This step is important 
for efficiency. 

If the tokenizer object is not created, when the text is passed throught the nlp object, all of the features of the model would be created, i.e., for each document, the code would 
generate tokenisation, part of speech tagging, dependency parsing, etc. For the large volume of data in this project, this would make the process much slower. However, by defining 
the tokenizer objcet, the function only applies the tokenizer to the text, thus making the function more efficient. 

The tokenizer performs an initial basic cleaning (removes special break/newline characters '\n\t' etc) and any extra white spaces.

The tokenizer then calls the 'contractions' library to expand the contractions in the text. This means it transforms "I've" to "I have". This helps to make the text 
more uniform, so words like 'I've' and ' I have' are represented consistently in the vector space (once passed thru the vectorizer).
Using SpaCy only (without this step) would simply transform "I've" to "I" "'ve". This means the text would have a lot of contraction fragments (such as 'nt 've etc) which might not occur as often
and could act as noise to the model.  

Then once this is complete, the model tokenizes all the text using the SpaCy tokenizer. It removes all punctuation marks (if it exsits between letters as well) and numeric values. 

At the end, the text is transformed into a cleaned list of lower case tokens. 

This tokenizer performed better in comparison to None and the other custom tokenizers. Using a pre trained model like SpaCy allows for the nunances of the English language
to be captured when splitting words into tokens. In addition, the contraction expansion also helped to make the content of the bag of words more consisent and uniform. The hyperparameter 
class_weight was also set to 'balanced' to account for the imbalances in the data set. Despite this tokenizer being slightly more computationally expensive, the boost in performance
for the SVC model from approx 0.65~0.66 to 0.70 appears to be worthwhile. 