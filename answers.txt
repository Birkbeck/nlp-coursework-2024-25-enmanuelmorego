Answers to the text questions go here.

Part 2
Question E

Various tokenizers with different functionality were tested and after evaluation, my_tokenizer_contractions_clean() provided the best balance between performance and efficiency.

The tokenizer uses the pre-trained SpaCy model 'en_core_web_sm'. This contains a large list of English words along with their information (is the word a noun, verb, etc). It also 
contains specific tokenization rules, parsing, part of speech tagging etc. All of this is available in the nlp object that is created. 

The nlp object is created which holds all of the information and processing pipelines stored in 'en_core_web_sm'.

Then a custom tokenier is created using the SpaCy Tokenizer and the rules in the 'en_core_wb_sm' model. This tokenizer object is used later to tokenize the text. This step is important 
for efficiency. 

If the tokenizer object is not created, when the text is passed throught the nlp object, all of the features of the model would be created, i.e., for each document, the code would 
generate tokenisation, part of speech tagging, dependency parsing, etc. For the large volume of data in this project, this would make the process much slower. However, by defining 
the tokenizer objcet, the function only applies the tokenizer to the text, thus making the function more efficient. 

The tokenizer performs an initial basic cleaning (removes special break/newline characters '\n\t' etc) and any extra white spaces.

The tokenizer then calls the 'contractions' library to expand the contractions in the text. This means it transforms "I've" to "I have". This helps to make the text 
more uniform, so words like 'I've' and ' I have' are represented consistently in the vector space (once passed thru the vectorizer).
Using SpaCy only (without this step) would simply transform "I've" to "I" "'ve". This means the text would have a lot of contraction fragments (such as 'nt 've etc) which might not occur as often
and could act as noise to the model.  

Then once this is complete, the model tokenizes all the text using the SpaCy tokenizer. It removes all punctuation marks (if it exsits between letters as well) and numeric values. 

At the end, the text is transformed into a cleaned list of lower case tokens. 

This tokenizer performed better in comparison to None and the other custom tokenizers. Using a pre trained model like SpaCy allows for the nunances of the English language
to be captured when splitting words into tokens. In addition, the contraction expansion also helped to make the content of the bag of words more consisent and uniform. The hyperparameter 
class_weight was also set to 'balanced' to account for the imbalances in the data set. Despite this tokenizer being slightly more computationally expensive, the boost in performance
for the SVC model from approx 0.65~0.66 to 0.70 appears to be worthwhile. 