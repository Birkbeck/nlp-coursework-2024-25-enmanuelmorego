{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f596f013",
   "metadata": {},
   "source": [
    "# Part Two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904a28c9",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134796c9",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "670483ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import re\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3808e0e0",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7404d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_speeches_csv(path=Path.cwd() / \"texts\" / \"p2-texts\"):\n",
    "    '''\n",
    "    Function to load csv files into pandas data frames\n",
    "\n",
    "    Args:\n",
    "        Function defaults to a specific location to search for the files unless otherwise specified\n",
    "\n",
    "    Returns\n",
    "        Pandas data frame\n",
    "    '''\n",
    "    # Extract file name\n",
    "    file = os.listdir(path)[0]\n",
    "    file_load = os.path.join(path, file)\n",
    "\n",
    "    # Read data\n",
    "    df = pd.read_csv(file_load)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaadf89",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3dbf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speeches_clean(df):\n",
    "    '''\n",
    "    Function that takes a data frame containing speeches, and performs custom cleaning tasks on it\n",
    "    Custom cleaning tasks are:\n",
    "        - Column 'party': replaces all entries 'Labour (Co-op)' with 'Labour'\n",
    "        - Column 'party': removes all values where entry is 'Speaker'\n",
    "        - Column 'party': only keeps the rows of the four most common parties\n",
    "            Find the frequency count for each party, and keep the top 4 only\n",
    "        - Column 'speech_class': removes all rows where value is NOT 'Speech'\n",
    "        - Column 'speech': removes any entries where the length of the speech is less than 1000 characters\n",
    "\n",
    "    Args: \n",
    "        df: Pandas data frame\n",
    "\n",
    "    Returns:\n",
    "        A Pandas data frame, cleaned\n",
    "    '''\n",
    "    # (a).i Clean Labour (Co-op) values\n",
    "    df_cleaned = df.replace('Labour (Co-op)', 'Labour')\n",
    "\n",
    "    # (a).ii Remove rows where 'party' == 'Speaker'\n",
    "    '''Note: Remove speaker rows first, otherwise this will interfere with finding the most common parties'''\n",
    "    df_cleaned = df_cleaned[df_cleaned['party'] != 'Speaker']\n",
    "\n",
    "    # (a).ii Remove rows where the value of 'party' is not one of the 4 most common parties\n",
    "    parties_count = df_cleaned['party'].value_counts().sort_values(ascending=False)\n",
    "    # # Extract the name of the 4 most common parties \n",
    "    top4_parties = parties_count.index[:4].tolist()\n",
    "    # # Filter to top 4 most common parties\n",
    "    df_cleaned2 = df_cleaned[df_cleaned['party'].isin(top4_parties)]\n",
    "\n",
    "    # (a).iii Remove rows where value in 'speech_class' is not 'Speech\n",
    "    df_cleaned2 = df_cleaned2[df_cleaned2['speech_class'] == 'Speech']\n",
    "\n",
    "    return df_cleaned2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ccf26",
   "metadata": {},
   "source": [
    "### Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "976df2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_pipeline(**kwargs):\n",
    "    '''\n",
    "    Function which processes and build ML models given the speeches data and prepares the data to be fed into ML models:\n",
    "    The pipeline:\n",
    "        Splits into train, test sets\n",
    "        Vectorises the data\n",
    "        Trains a RandomForest Model\n",
    "        Trains a Linear SVM classifer\n",
    "        Extracts the CLassification Report for each model\n",
    "        Macro-Average F1 Score\n",
    "    \n",
    "    Arguments can be passed as key value pairs. Some arguments are mandatory whilst other are optionals. When optional arguments are not provided\n",
    "    the function will use defaul values\n",
    "    Ars:\n",
    "        data (mandatory): A cleaned pandas data frame\n",
    "        ngram (optional): a tuple containing the ngram to consider to pass in the TfidVectorizer function\n",
    "            default value: (1,1) unigrams\n",
    "        stop_words (optional): A string containing the value for the stop_words argument for TfidVectorizer. If set ti 'english', stop words would be removed\n",
    "            default value: None - Stop words would not be removed\n",
    "        class_weights (optional): balances the weight for each class in the model depending on frequency counts\n",
    "\n",
    "    '''\n",
    "    # Extract input parameters\n",
    "    input_dict = kwargs\n",
    "\n",
    "    # Extract data from input\n",
    "    df = input_dict.get('data')\n",
    "    ngram = input_dict.get('ngram', (1,1))\n",
    "    stop_words = input_dict.get('stop_words', None)\n",
    "    tokenizer = input_dict.get('tokenizer', None)\n",
    "    class_weight = input_dict.get('class_weight', None)\n",
    "\n",
    "    # Tokenizer print: \n",
    "    if tokenizer is not None:\n",
    "        token_print = tokenizer.__name__\n",
    "    else:\n",
    "        token_print = tokenizer\n",
    "    print(\"\\nArguments:\")\n",
    "    print(f\"\\tNgram: {ngram}\\n\\tStop words: {stop_words}\\n\\tTokenizer: {token_print}\\n\\tClass Weights: {class_weight}\")\n",
    "\n",
    "    # (b) Generate object that splits data using stratified sampling, and random seed of 26\n",
    "    splitter_obj = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 26) \n",
    "    # Split data\n",
    "    for train_index, test_index in splitter_obj.split(df, df['party']):\n",
    "        train = df.iloc[train_index]\n",
    "        test = df.iloc[test_index]\n",
    "\n",
    "    # (b) Split target in both training and testing set\n",
    "    y_train, y_test = train['party'], test['party']\n",
    "\n",
    "    # (b) Create vectorised data for x objects\n",
    "    '''\n",
    "    Max features set to 3000\n",
    "    stop_words, ngram = defined by parameters when function is called\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(max_features = 3000, \n",
    "                                 stop_words=stop_words, \n",
    "                                 ngram_range = ngram,\n",
    "                                 tokenizer = tokenizer)\n",
    "    x_train = vectorizer.fit_transform(train['speech'])\n",
    "    x_test = vectorizer.transform(test['speech'])\n",
    "\n",
    "    # (c) Train random forest\n",
    "    random_forest = RandomForestClassifier(n_estimators=300, n_jobs = -1, class_weight=class_weight) \n",
    "    random_forest.fit(x_train, y_train)\n",
    "    random_forest_y_predict = random_forest.predict(x_test)\n",
    "\n",
    "    # (c) Train SVM\n",
    "    svm = LinearSVC(class_weight=class_weight)\n",
    "    svm.fit(x_train, y_train)\n",
    "    svm_y_predict = svm.predict(x_test)\n",
    "\n",
    "    # Get label names\n",
    "    target_names = y_test.unique()\n",
    "\n",
    "    # Results section \n",
    "    print(f\"{\"=\"*20} Random Forest Performance {\"=\"*20}\")\n",
    "    rf_cr = classification_report(y_test, random_forest_y_predict, target_names = target_names, output_dict = True)\n",
    "    print(classification_report(y_test, random_forest_y_predict, target_names = target_names))\n",
    "\n",
    "    print(f\"{\"=\"*20} SVC Performance {\"=\"*20}\")\n",
    "    svc_cr = classification_report(y_test, svm_y_predict, target_names = target_names, output_dict = True)\n",
    "    print(classification_report(y_test, svm_y_predict, target_names = target_names))\n",
    "\n",
    "    return {'rf': rf_cr, 'svc': svc_cr}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf240696",
   "metadata": {},
   "source": [
    "### Custom Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4865d6a",
   "metadata": {},
   "source": [
    "#### Basic Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer_basic(text):\n",
    "    '''\n",
    "    Basic tokenizer:\n",
    "        Removes special break characters, such as \\n, \\t etc\n",
    "        Removes any extra white spaces \n",
    "        Uses nltk word tokenizer to split the words into objects\n",
    "        Only keeps alphabetical objects, ignores numeric and punctuation marks\n",
    "        It keeps enligh stop words\n",
    "    '''\n",
    "    # Clean the text. Remove special characters, such as \\n, \\t etc and extra white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [token for token in tokens if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3f025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'text', 'to', 'test', 'the', 'tokenizer', 'we', 'add', 'contractions', 'such', 'as', 'dont', 'wo', 'and', 'punctuation', 'to', 'test', 'how', 'the', 'tokenizer', 'handles', 'these', 'tokenizing', 'ml', 'also', 'we', 'check', 'for', 'prime', 'minister', 'the', 'speaker', 'mr', 'speaker', 'and', 'see', 'how', 'these', 'are', 'treated', 'too', 'along', 'with', 'numeric', 'values']\n"
     ]
    }
   ],
   "source": [
    "'''Test tokenizer\n",
    "    - How does it handle contractions: Don't, dont, let's, lets\n",
    "    - How does it handle name objects: Prime Minister, Chris, The Conservative Party\n",
    "    - Special characters: \\n\\t\n",
    "'''\n",
    "test_text = \"test tokenizer\\n\\t. contractions!, SUCH as dont, won't, co-operate and punctuation? how the tokenizer handles these? #tokenizing #ml. Also, we check for Prime Minister, the speaker, mr Speaker and see how these are treated too, along with numeric values 1000 1,000 01/01/2020\"\n",
    "\n",
    "print(my_tokenizer_basic(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212a6314",
   "metadata": {},
   "source": [
    "#### Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e7640",
   "metadata": {},
   "source": [
    "The function below tries a sentence tokenizer. By capturing the full embedded meaning of a sentence instead of a word itself, it is hoped that the model has access to more contextual information and might be able to better predict party based on speech. It is possible that sentences can carry more contextual information than specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1fe02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer_sentence(text):\n",
    "    '''\n",
    "    Sentence tokenizer:\n",
    "        Removes special break characters, such as \\n, \\t etc\n",
    "        Removes any extra white spaces \n",
    "        Uses nltk sentence tokenizer to split the senteces into objects\n",
    "    '''\n",
    "    # Clean the text. Remove special characters, such as \\n, \\t etc and extra white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63ce6ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a text to test the tokenizer .', \"We add contractions!, such as dont, won't, co-operate and punctuation?\", 'to test how the tokenizer handles these #tokenizing #ml.', 'Also, we check for Prime Minister, the speaker, mr Speaker and see how these are treated too, along with numeric values 1000 1,000 01/01/2020']\n"
     ]
    }
   ],
   "source": [
    "# Test tokenizer\n",
    "print(my_tokenizer_sentence(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8fc2e",
   "metadata": {},
   "source": [
    "From the examples above it seems that we need a tokenizer that provides cleaner data. For example, Prime Minister would be one object. If we have sentence such as 'Prime Minister's prime objective is to speak with the ministers...' we would be counting prime twice, but in reality, perhaps we should count Prime-Minister as a named entity, and prime as an adjective. \n",
    "\n",
    "Another issue is contractions. There could be instances where the text is: \"we're\" and \"we are\", despite these two objects being the same, the vectorizer would encode these as two separate items. This increases the dimensionality of the data, and makes the vectorization process less precise. \n",
    "\n",
    "The custom tokenizer will attempt to handle these cases.\n",
    "\n",
    "Whilst the solution is not perfect, it is expected that with the cleaning, and the inherent limitations of these techniques, the overall prediction power would increase\n",
    "\n",
    "We will try to achieve such level of specificity by calling `Spacy` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce44be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# def my_tokenizer_spacy(text):\n",
    "    \n",
    "\n",
    "#     doc = nlp(text)\n",
    "#     for token in doc:\n",
    "#         print(token)\n",
    "#     return token_list\n",
    "\n",
    "import pycontractions\n",
    "# Clean contractions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06b96a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "tokenizer\n",
      "\n",
      "\t\n",
      ".\n",
      "contractions\n",
      "!\n",
      ",\n",
      "SUCH\n",
      "as\n",
      "do\n",
      "nt\n",
      ",\n",
      "wo\n",
      "n't\n",
      ",\n",
      "co\n",
      "-\n",
      "operate\n",
      "and\n",
      "punctuation\n",
      "?\n",
      "how\n",
      "the\n",
      "tokenizer\n",
      "handles\n",
      "these\n",
      "?\n",
      "#\n",
      "tokenizing\n",
      "#\n",
      "ml\n",
      ".\n",
      "Also\n",
      ",\n",
      "we\n",
      "check\n",
      "for\n",
      "U.K.\n",
      "Prime\n",
      "Minister\n",
      ",\n",
      "the\n",
      "speaker\n",
      ",\n",
      "mr\n",
      "Speaker\n",
      "and\n",
      "see\n",
      "how\n",
      "these\n",
      "are\n",
      "treated\n",
      "too\n",
      ",\n",
      "along\n",
      "with\n",
      "numeric\n",
      "values\n",
      "1000\n",
      "1,000\n",
      "01/01/2020\n"
     ]
    }
   ],
   "source": [
    "test_text = \"test tokenizer\\n\\t. contractions!, SUCH as dont, won't, co-operate and punctuation? how the tokenizer handles these? #tokenizing #ml. Also, we check for U.K. Prime Minister, the speaker, mr Speaker and see how these are treated too, along with numeric values 1000 1,000 01/01/2020\"\n",
    "\n",
    "doc = nlp(test_text)\n",
    "\n",
    "improved_list = []\n",
    "for i in range(0, len(doc)):\n",
    "    print(doc[i].text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e18ac3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'tokenizer', '\\n\\t', '.', 'contractions', '!', ',', 'SUCH', 'as', 'do', 'nt', ',', 'wo', \"n't\", ',', 'co', '-', 'operate', 'and', 'punctuation', '?', 'how', 'the', 'tokenizer', 'handles', 'these', '?', '#', 'tokenizing', '#', 'ml', '.', 'Also', ',', 'we', 'check', 'for', 'U.K.', 'Prime', 'Minister', ',', 'the', 'speaker', ',', 'mr', 'Speaker', 'and', 'see', 'how', 'these', 'are', 'treated', 'too', ',', 'along', 'with', 'numeric', 'values', '1000', '1,000', '01/01/2020']\n"
     ]
    }
   ],
   "source": [
    "plain_list = []\n",
    "\n",
    "for token in doc:\n",
    "    plain_list.append(token.text)\n",
    "\n",
    "print(plain_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54d9019",
   "metadata": {},
   "source": [
    "## Program / Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c88295",
   "metadata": {},
   "source": [
    "### Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8d92e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36223, 8)\n"
     ]
    }
   ],
   "source": [
    " # Load speeches data frame\n",
    "df = read_speeches_csv()\n",
    "# Clean data frame\n",
    "df_cleaned = speeches_clean(df)\n",
    "# Print dimensions\n",
    "print(df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c1a24e",
   "metadata": {},
   "source": [
    "See the class distribution below. It appears that the dataset is imbalanced, in the sense that there is a vast majority of entries for the Conservative party, and a very small proportion of entries for Liberal Democrat. This can have an impact on the classifiers, and should be addressed.\n",
    "\n",
    "SK_Learn provides the class_weight argument, which can be passed to our models. By providing the value 'balanced', the model builds a dictionary where the weights are proportional to the class (similar idea to stratified sampling, in the way that we don't want to over-represent a particular party simply because it appears more often)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c72bf914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "party\n",
      "Conservative               25079\n",
      "Labour                      8038\n",
      "Scottish National Party     2303\n",
      "Liberal Democrat             803\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned['party'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca9905",
   "metadata": {},
   "source": [
    "### Train and test ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d91d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to record the Macro Avg F1 score for each tested model\n",
    "f1_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef754d",
   "metadata": {},
   "source": [
    "#### Model set 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2bb5bc",
   "metadata": {},
   "source": [
    "Train a Random Forest Model and SVM linear Kernel model:\n",
    "\n",
    "    Remove English stop word: Yes\n",
    "    Ngram: unigram only\n",
    "    Tokenizer: Default\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b62231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arguments:\n",
      "\tNgram: (1, 1)\n",
      "\tStop words: english\n",
      "\tTokenizer: None\n",
      "\tClass Weights: None\n",
      "==================== Random Forest Performance ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels wi