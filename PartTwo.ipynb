{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f596f013",
   "metadata": {},
   "source": [
    "# Part Two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904a28c9",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134796c9",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "670483ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import contractions\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3808e0e0",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7404d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_speeches_csv(path=Path.cwd() / \"texts\" / \"p2-texts\"):\n",
    "    '''\n",
    "    Function to load csv files into a pandas data frame\n",
    "\n",
    "    Args:\n",
    "        Function defaults to a specific location to search for the files unless otherwise specified\n",
    "\n",
    "    Returns\n",
    "        Pandas data frame\n",
    "    '''\n",
    "    # Extract file name\n",
    "    file = os.listdir(path)[0]\n",
    "    file_load = os.path.join(path, file)\n",
    "\n",
    "    # Read data\n",
    "    df = pd.read_csv(file_load)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaadf89",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7a3dbf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speeches_clean(df):\n",
    "    '''\n",
    "    Function that takes a data frame containing speeches, and performs custom cleaning tasks on it\n",
    "    Custom cleaning tasks are:\n",
    "        - Column 'party': replaces all entries 'Labour (Co-op)' with 'Labour'\n",
    "        - Column 'party': removes all values where entry is 'Speaker'\n",
    "        - Column 'party': only keeps the rows of the four most common parties\n",
    "                          Finds the frequency count for each party, and keep the top 4 only\n",
    "        - Column 'speech_class': removes all rows where value is NOT 'Speech'\n",
    "        - Column 'speech': removes any entries where the length of the speech is less than 1000 characters\n",
    "\n",
    "    Args: \n",
    "        df: Pandas data frame\n",
    "\n",
    "    Returns:\n",
    "        A Pandas data frame, cleaned\n",
    "    '''\n",
    "    # (a).i Clean Labour (Co-op) values\n",
    "    df_cleaned = df.replace('Labour (Co-op)', 'Labour')\n",
    "\n",
    "    # (a).ii Remove rows where 'party' == 'Speaker'\n",
    "    '''Note: Remove speaker rows first, otherwise this will interfere with finding the most common parties'''\n",
    "    df_cleaned = df_cleaned[df_cleaned['party'] != 'Speaker']\n",
    "\n",
    "    # (a).ii Remove rows where the value of 'party' is not one of the 4 most common parties\n",
    "    parties_count = df_cleaned['party'].value_counts().sort_values(ascending=False)\n",
    "    # # Extract the name of the 4 most common parties \n",
    "    top4_parties = parties_count.index[:4].tolist()\n",
    "    # # Filter to top 4 most common parties\n",
    "    df_cleaned2 = df_cleaned[df_cleaned['party'].isin(top4_parties)]\n",
    "\n",
    "    # (a).iii Remove rows where value in 'speech_class' is not 'Speech\n",
    "    df_cleaned2 = df_cleaned2[df_cleaned2['speech_class'] == 'Speech']\n",
    "\n",
    "    # (a).iv Remove rows where the text in speech columns is less than 1000\n",
    "    df_out = df_cleaned2[df_cleaned2['speech'].str.len() >= 1000]\n",
    "\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ccf26",
   "metadata": {},
   "source": [
    "### Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976df2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_pipeline(**kwargs):\n",
    "    '''\n",
    "    Function which processes and build ML models given the speeches data and prepares the data to be fed into ML models:\n",
    "    The pipeline:\n",
    "        Splits into train, test sets\n",
    "        Vectorises the data\n",
    "        Trains a RandomForest Model\n",
    "        Trains a Linear SVM classifer\n",
    "        Extracts the CLassification Report for each model\n",
    "        Macro-Average F1 Score\n",
    "    \n",
    "    Arguments can be passed as key value pairs. Some arguments are mandatory whilst other are optionals. When optional arguments are not provided\n",
    "    the function will use defaul values\n",
    "    Ars:\n",
    "        data (mandatory): A cleaned pandas data frame\n",
    "        ngram (optional): A tuple containing the ngram to consider to pass in the TfidVectorizer function\n",
    "                          default value: (1,1) unigrams\n",
    "        stop_words (optional): A string containing the value for the stop_words argument for TfidVectorizer. If set ti 'english', stop words would be removed\n",
    "                               default value: None - Stop words would not be removed\n",
    "        class_weights (optional): Balances the weight for each class in the model depending on frequency counts\n",
    "\n",
    "    '''\n",
    "    # Extract input parameters\n",
    "    input_dict = kwargs\n",
    "\n",
    "    # Extract data from input\n",
    "    df = input_dict.get('data')\n",
    "    ngram = input_dict.get('ngram', (1,1))\n",
    "    stop_words = input_dict.get('stop_words', None)\n",
    "    tokenizer = input_dict.get('tokenizer', None)\n",
    "    class_weight = input_dict.get('class_weight', None)\n",
    "\n",
    "    # Tokenizer print object \n",
    "    if tokenizer is not None:\n",
    "        token_print = tokenizer.__name__\n",
    "    else:\n",
    "        token_print = tokenizer\n",
    "    print(\"\\nArguments:\")\n",
    "    print(f\"\\tNgram: {ngram}\\n\\tStop words: {stop_words}\\n\\tTokenizer: {token_print}\\n\\tClass Weights: {class_weight}\")\n",
    "\n",
    "    # (b) Generate object that splits data using stratified sampling, and random seed of 26\n",
    "    splitter_obj = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 26) \n",
    "    # Split data\n",
    "    for train_index, test_index in splitter_obj.split(df, df['party']):\n",
    "        train = df.iloc[train_index]\n",
    "        test = df.iloc[test_index]\n",
    "\n",
    "    # (b) Split target in both training and testing set\n",
    "    y_train, y_test = train['party'], test['party']\n",
    "\n",
    "    # (b) Create vectorised data for x objects\n",
    "    '''\n",
    "    Fixed Arguments (required by asignment promt)\n",
    "        Max features: set to 3000\n",
    "        Stop words: set to Enlish\n",
    "\n",
    "    Flexible Arguments\n",
    "        stop_words: defined by parameters when function is called (Set to default value if not specified)\n",
    "        ngram: defined by parameters when function is called (Set to default value if not specified)\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(max_features = 3000, \n",
    "                                 stop_words=stop_words, \n",
    "                                 ngram_range = ngram,\n",
    "                                 tokenizer = tokenizer)\n",
    "    x_train = vectorizer.fit_transform(train['speech'])\n",
    "    x_test = vectorizer.transform(test['speech'])\n",
    "\n",
    "    # (c) Train random forest\n",
    "    random_forest = RandomForestClassifier(n_estimators=300, n_jobs = -1, class_weight=class_weight) \n",
    "    random_forest.fit(x_train, y_train)\n",
    "    random_forest_y_predict = random_forest.predict(x_test)\n",
    "\n",
    "    # (c) Train SVM\n",
    "    svm = LinearSVC(class_weight=class_weight)\n",
    "    svm.fit(x_train, y_train)\n",
    "    svm_y_predict = svm.predict(x_test)\n",
    "\n",
    "    # Get label names\n",
    "    target_names = y_test.unique()\n",
    "\n",
    "    # Results section \n",
    "    print(f\"{\"=\"*20} Random Forest Performance {\"=\"*20}\")\n",
    "    rf_cr = classification_report(y_test, random_forest_y_predict, target_names = target_names, output_dict = True)\n",
    "    print(classification_report(y_test, random_forest_y_predict, target_names = target_names))\n",
    "\n",
    "    print(f\"{\"=\"*20} SVC Performance {\"=\"*20}\")\n",
    "    svc_cr = classification_report(y_test, svm_y_predict, target_names = target_names, output_dict = True)\n",
    "    print(classification_report(y_test, svm_y_predict, target_names = target_names))\n",
    "\n",
    "    return {'rf': rf_cr, 'svc': svc_cr}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf240696",
   "metadata": {},
   "source": [
    "### Custom Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4865d6a",
   "metadata": {},
   "source": [
    "#### Basic Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe4e1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer_basic(text):\n",
    "    '''\n",
    "    Basic tokenizer:\n",
    "        Removes special break characters, such as \\n, \\t etc\n",
    "        Removes any extra white spaces \n",
    "        Uses nltk word tokenizer to split the words into objects\n",
    "        Only keeps alphabetical objects, ignores numeric and punctuation marks\n",
    "        It keeps enligh stop words\n",
    "    '''\n",
    "    # Clean the text. Remove special characters, such as \\n, \\t etc and extra white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [token for token in tokens if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32d3f025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'tokenizer', 'contractions', 'such', 'as', 'dont', 'wo', 'and', 'punctuation', 'how', 'the', 'tokenizer', 'handles', 'these', 'tokenizing', 'ml', 'also', 'we', 'check', 'for', 'prime', 'minister', 'the', 'speaker', 'mr', 'speaker', 'and', 'see', 'how', 'these', 'are', 'treated', 'too', 'along', 'with', 'numeric', 'values']\n"
     ]
    }
   ],
   "source": [
    "'''Test tokenizer\n",
    "    - How does it handle contractions: Don't, dont, let's, lets\n",
    "    - How does it handle name objects: Prime Minister, Chris, The Conservative Party\n",
    "    - Special characters: \\n\\t\n",
    "'''\n",
    "test_text = \"test tokenizer\\n\\t. contractions!, SUCH as dont, won't, co-operate and punctuation? how the tokenizer handles these? #tokenizing #ml. Also, we check for Prime Minister, the speaker, mr Speaker and see how these are treated too, along with numeric values 1000 1,000 01/01/2020\"\n",
    "\n",
    "print(my_tokenizer_basic(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212a6314",
   "metadata": {},
   "source": [
    "#### Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e7640",
   "metadata": {},
   "source": [
    "The function below tries a sentence tokenizer. By capturing the full embedded meaning of a sentence instead of a word itself, it is hoped that the model has access to more contextual information and might be able to better predict party based on speech. It is possible that sentences can carry more contextual information than specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1fe02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer_sentence(text):\n",
    "    '''\n",
    "    Sentence tokenizer:\n",
    "        Removes special break characters, such as \\n, \\t etc\n",
    "        Removes any extra white spaces \n",
    "        Uses nltk sentence tokenizer to split the senteces into objects\n",
    "    '''\n",
    "    # Clean the text. Remove special characters, such as \\n, \\t etc and extra white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63ce6ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test tokenizer .', \"contractions!, SUCH as dont, won't, co-operate and punctuation?\", 'how the tokenizer handles these?', '#tokenizing #ml.', 'Also, we check for Prime Minister, the speaker, mr Speaker and see how these are treated too, along with numeric values 1000 1,000 01/01/2020']\n"
     ]
    }
   ],
   "source": [
    "# Test tokenizer\n",
    "print(my_tokenizer_sentence(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8fc2e",
   "metadata": {},
   "source": [
    "From the examples above it seems that we need a tokenizer that provides cleaner data. \n",
    "\n",
    "The tokenizer below attempts to solve the following issues:\n",
    "\n",
    "- Words contractions:\n",
    "    Words like `won't` would be stored in a different vector than `will` and `not`, despite having the same meaning. The tokenizer below uses the `contraction` library to expand these contractions into a uniform output. Anytime `won't` is encountered, it gets transformed to `will not`. This consistency across the corpus might provide more accurate representation and improve the performance of the model.\n",
    "\n",
    "- Named entities:\n",
    "    Names entities like `Prime Minister` would be stored as two vectors, one for `prime` and one for `minister`. If in the corpus the word `prime` appears in a different context, such as a discussion on `prime TV shows`, the word `prime` would get a frequency count of 2, despite these two words having very different meaning given their context. The tokenizer below uses `Spacy` `en_core_web_lg` to detect all name entities, and when found, these are collapsed into a single word, i.e., `primeminister`. The objective is that each time `Prime Minister` appears, it is encoded as the entity of `Prime Minister` and not two separate words. \n",
    "    This should also help with stop words, as `The Church` would become `thechurch` as it refers to a specific entity, thus increasing the resolution and context of stop words as well. \n",
    "\n",
    "- Standard cleaning:\n",
    "    This tokenizer also applies standard cleaning, such as converting all words to lower case, removes punctuation marks and digits. \n",
    "\n",
    "- Stop words:\n",
    "    Stop words are not removed as these might be significant and perhaps could be part of named entities.\n",
    "\n",
    "Unfortunately, this complex tokenization did not add any predictive power to our models. In fact, it performed worse than some simpler tokenizers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ce44be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def my_tokenizer_spacy(text):\n",
    "    \n",
    "\n",
    "    # clean special chatacters and remove extra spaces \n",
    "    text_trimmed = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove extra spaces and transform to lower case\n",
    "    text_trimmed = text_trimmed.strip()\n",
    "    # Do initial simple split\n",
    "    token_iter = text_trimmed.split()\n",
    "\n",
    "    # fix contractions only for words with '\n",
    "    fixed_contractions = [] \n",
    "    for word in token_iter:\n",
    "        if \"'\" in word:\n",
    "            fixed_contractions.append(contractions.fix(word))\n",
    "        else:\n",
    "            fixed_contractions.append(word)\n",
    "    # Join back to string\n",
    "    text_string = \" \".join(fixed_contractions)\n",
    "\n",
    "    # Pass spacy parser\n",
    "    doc = nlp(text_string)\n",
    "    tokenized = []\n",
    "    processed_token_indices = []\n",
    "    # First, save named entities for accuracy (see text above for explanation)\n",
    "    '''Loop using indeces, and save index number to not double count objects in tokenizer'''\n",
    "    for ent in doc.ents:\n",
    "        # Join named entities \n",
    "        ent_clean = re.sub(r\"[^\\w\\s]\", \"\", ent.text).replace(\" \", \"\").lower()\n",
    "        if ent_clean.isalpha():\n",
    "            tokenized.append(ent_clean)\n",
    "        for token in ent:\n",
    "            processed_token_indices.append(token.i)\n",
    "\n",
    "    # Loop over document to extract words, without double counting the already seen values\n",
    "    for token in doc:\n",
    "        if token.i not in processed_token_indices:\n",
    "        # Clean punctuation marks in words (if any)\n",
    "            cleaned_token = re.sub(r\"[^\\w\\s]\", \"\", token.text)\n",
    "            # Then only append letters\n",
    "            if cleaned_token.isalpha():\n",
    "                tokenized.append(cleaned_token.lower())\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45d61852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'tokenizer', 'contractions', 'such', 'as', 'do', 'not', 'will', 'not', 'cooperate', 'and', 'punctuation', 'how', 'the', 'tokenizer', 'handles', 'these', 'tokenizing', 'ml', 'also', 'we', 'check', 'for', 'uk', 'prime', 'minister', 'the', 'speaker', 'mr', 'speaker', 'and', 'see', 'how', 'these', 'are', 'treated', 'too', 'along', 'with', 'numeric', 'values']\n"
     ]
    }
   ],
   "source": [
    "from spacy import tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "nlp = English()\n",
    "my_custom_tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "def my_tokenizer_contractions_clean(text):\n",
    "        \n",
    "    # clean special chatacters and remove extra spaces \n",
    "    text_trimmed = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove extra spaces and transform to lower case\n",
    "    text_trimmed = text_trimmed.strip()\n",
    "    # Do initial simple split\n",
    "    token_iter = text_trimmed.split()\n",
    "\n",
    "    # fix contractions only for words with '\n",
    "    fixed_contractions = [] \n",
    "    for word in token_iter:\n",
    "        if \"'\" in word:\n",
    "            fixed_contractions.append(contractions.fix(word))\n",
    "        else:\n",
    "            fixed_contractions.append(word)\n",
    "    # Join back to string\n",
    "    text_string = \" \".join(fixed_contractions)\n",
    "\n",
    "    tokenized = my_custom_tokenizer(text_string)\n",
    "    tokenized_out = []\n",
    "    for token in tokenized:\n",
    "        # Clean punctuation marks in words (if any)\n",
    "        cleaned_token = re.sub(r\"[^\\w\\s]\", \"\", token.text)\n",
    "\n",
    "        if cleaned_token.isalpha():\n",
    "            tokenized_out.append(cleaned_token.lower())\n",
    "\n",
    "    return tokenized_out\n",
    "\n",
    "text = \"test tokenizer\\n\\t. contractions!, SUCH as don't, won't, co-operate and punctuation? how the tokenizer handles these? #tokenizing #ml. Also, we check for U.K. Prime Minister, the speaker, mr Speaker and see how these are treated too, along with numeric values 1000 1,000 01/01/2020\"\n",
    "print(my_tokenizer_contractions_clean(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "44355e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'tokenizer', 'contraction', 'such', 'as', 'do', 'will', 'co', 'operate', 'and', 'punctuation', 'how', 'the', 'tokenizer', 'handle', 'these', 'tokenize', 'ml', 'also', 'we', 'check', 'for', 'prime', 'minister', 'the', 'speaker', 'mr', 'speaker', 'and', 'see', 'how', 'these', 'be', 'treat', 'too', 'along', 'with', 'numeric', 'value']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "\n",
    "def my_tokenizer_lemma(text):\n",
    "        \n",
    "    # clean special chatacters and remove extra spaces \n",
    "    text_trimmed = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove extra spaces and transform to lower case\n",
    "    text_trimmed = text_trimmed.strip()\n",
    "\n",
    "    # tokenized = my_custom_tokenizer(text_string)\n",
    "    doc = nlp(text_trimmed)\n",
    "    tokenized_out = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.isalpha():\n",
    "            tokenized_out.append(token.lemma_.lower())\n",
    "\n",
    "    return tokenized_out\n",
    "\n",
    "text = \"test tokenizer\\n\\t. contractions!, SUCH as don't, won't, co-operate and punctuation? how the tokenizer handles these? #tokenizing #ml. Also, we check for U.K. Prime Minister, the speaker, mr Speaker and see how these are treated too, along with numeric values 1000 1,000 01/01/2020\"\n",
    "print(my_tokenizer_lemma(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54d9019",
   "metadata": {},
   "source": [
    "## Program / Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c88295",
   "metadata": {},
   "source": [
    "### Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8d92e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8084, 8)\n"
     ]
    }
   ],
   "source": [
    " # Load speeches data frame\n",
    "df = read_speeches_csv()\n",
    "# Clean data frame\n",
    "df_cleaned = speeches_clean(df)\n",
    "# Print dimensions\n",
    "print(df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c1a24e",
   "metadata": {},
   "source": [
    "See the class distribution below. It appears that the dataset is imbalanced, in the sense that there is a vast majority of entries for the Conservative party, and a very small proportion of entries for Liberal Democrat. This can have an impact on the classifiers, and should be addressed.\n",
    "\n",
    "SK_Learn provides the class_weight argument, which can be passed to our models. By providing the value 'balanced', the model builds a dictionary where the weights are proportional to the class (similar idea to stratified sampling, in the way that we don't want to over-represent a particular party simply because it appears more often)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c72bf914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "party\n",
      "Conservative               4819\n",
      "Labour                     2317\n",
      "Scottish National Party     679\n",
      "Liberal Democrat            269\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned['party'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fca0cf",
   "metadata": {},
   "source": [
    "Once we found the best performing tokenizer, we can adjust and test various hyperparameters to improve the performance even more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99894f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#  (b) Generate object that splits data using stratified sampling, and random seed of 26\n",
    "splitter_obj = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 26) \n",
    "# Split data\n",
    "for train_index, test_index in splitter_obj.split(df_cleaned, df_cleaned['party']):\n",
    "    train = df_cleaned.iloc[train_index]\n",
    "    test = df_cleaned.iloc[test_index]\n",
    "# (b) Split target in both training and testing set\n",
    "y_train, y_test = train['party'], test['party']\n",
    "# (b) Create vectorised data for x objects\n",
    "'''\n",
    "Max features set to 3000\n",
    "stop_words, ngram = defined by parameters when function is called\n",
    "'''\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 3000, \n",
    "                             stop_words='english', \n",
    "                             ngram_range = (1,3),\n",
    "                             tokenizer = my_tokenizer_lemma)\n",
    "x_train = vectorizer.fit_transform(train['speech'])\n",
    "x_test = vectorizer.transform(test['speech'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bdc9e8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "[CV] END .............................C=0.001, max_iter=5000; total time=   0.5s\n",
      "[CV] END .............................C=0.001, max_iter=5000; total time=   0.5s\n",
      "[CV] END .............................C=0.001, max_iter=5000; total time=   0.5s\n",
      "[CV] END .............................C=0.001, max_iter=5000; total time=   0.5s\n",
      "[CV] END .............................C=0.001, max_iter=5000; total time=   0.5s\n",
      "[CV] END ..............................C=0.01, max_iter=5000; total time=   0.6s\n",
      "[CV] END ..............................C=0.01, max_iter=5000; total time=   0.7s\n",
      "[CV] END ..............................C=0.01, max_iter=5000; total time=   0.7s\n",
      "[CV] END ..............................C=0.01, max_iter=5000; total time=   0.5s\n",
      "[CV] END ..............................C=0.01, max_iter=5000; total time=   0.5s\n",
      "[CV] END ...............................C=0.1, max_iter=5000; total time=   0.7s\n",
      "[CV] END ...............................C=0.1, max_iter=5000; total time=   0.7s\n",
      "[CV] END ...............................C=0.1, max_iter=5000; total time=   0.7s\n",
      "[CV] END ...............................C=0.1, max_iter=5000; total time=   0.7s\n",
      "[CV] END ...............................C=0.1, max_iter=5000; total time=   0.7s\n",
      "[CV] END .................................C=1, max_iter=5000; total time=   0.9s\n",
      "[CV] END .................................C=1, max_iter=5000; total time=   1.1s\n",
      "[CV] END .................................C=1, max_iter=5000; total time=   1.1s\n",
      "[CV] END .................................C=1, max_iter=5000; total time=   1.1s\n",
      "[CV] END .................................C=1, max_iter=5000; total time=   1.1s\n",
      "[CV] END ................................C=10, max_iter=5000; total time=   1.8s\n",
      "[CV] END ................................C=10, max_iter=5000; total time=   1.8s\n",
      "[CV] END ................................C=10, max_iter=5000; total time=   2.3s\n",
      "[CV] END ................................C=10, max_iter=5000; total time=   1.9s\n",
      "[CV] END ................................C=10, max_iter=5000; total time=   2.0s\n",
      "[CV] END ..............................C=1000, max_iter=5000; total time=   2.1s\n",
      "[CV] END ..............................C=1000, max_iter=5000; total time=   2.9s\n",
      "[CV] END ..............................C=1000, max_iter=5000; total time=   3.1s\n",
      "[CV] END ..............................C=1000, max_iter=5000; total time=   2.9s\n",
      "[CV] END ..............................C=1000, max_iter=5000; total time=   2.3s\n",
      "[CV] END ...............................C=100, max_iter=5000; total time=   6.8s\n",
      "[CV] END ...............................C=100, max_iter=5000; total time=   7.0s\n",
      "[CV] END ...............................C=100, max_iter=5000; total time=   7.2s\n",
      "[CV] END ...............................C=100, max_iter=5000; total time=   6.5s\n",
      "[CV] END ...............................C=100, max_iter=5000; total time=   6.7s\n",
      "\n",
      "=== All macro F1 scores ===\n",
      "C=0.001, macro F1: 0.192\n",
      "C=0.01, macro F1: 0.520\n",
      "C=0.1, macro F1: 0.671\n",
      "C=1, macro F1: 0.670\n",
      "C=10, macro F1: 0.638\n",
      "C=100, macro F1: 0.635\n",
      "C=1000, macro F1: 0.635\n",
      "Best params: {'C': 0.1, 'max_iter': 5000}\n",
      "Best macro F1 (CV): 0.670542051710732\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.86      0.90      0.88       964\n",
      "                 Labour       0.76      0.69      0.73       463\n",
      "       Liberal Democrat       0.52      0.48      0.50        54\n",
      "Scottish National Party       0.70      0.71      0.70       136\n",
      "\n",
      "               accuracy                           0.81      1617\n",
      "              macro avg       0.71      0.69      0.70      1617\n",
      "           weighted avg       0.81      0.81      0.81      1617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_initial = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "tuned = [0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68]\n",
    "param_grid = [{'C': c_initial,\n",
    "               'max_iter': [5000]}]\n",
    "\n",
    "svc = LinearSVC(class_weight='balanced')\n",
    "\n",
    "grid_search = GridSearchCV(estimator = svc,\n",
    "                           param_grid = param_grid,\n",
    "                           scoring = 'f1_macro',\n",
    "                           cv = 5,\n",
    "                           verbose = 2,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(\"\\n=== All macro F1 scores ===\")\n",
    "for mean, params in zip(grid_search.cv_results_['mean_test_score'], grid_search.cv_results_['params']):\n",
    "    print(f\"C={params['C']}, macro F1: {mean:.3f}\")\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best macro F1 (CV):\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate best model on your held-out test:\n",
    "best_svc = grid_search.best_estimator_\n",
    "y_pred = best_svc.predict(x_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca9905",
   "metadata": {},
   "source": [
    "### Train and test ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d91d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to record the Macro Avg F1 score for each tested model\n",
    "f1_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef754d",
   "metadata": {},
   "source": [
    "#### Model set 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2bb5bc",
   "metadata": {},
   "source": [
    "Train a Random Forest Model and SVM linear Kernel model:\n",
    "\n",
    "    Remove English stop word: Yes\n",
    "    Ngram: unigram only\n",
    "    Tokenizer: Default\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b62231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arguments:\n",
      "\tNgram: (1, 1)\n",
      "\tStop words: english\n",
      "\tTokenizer: None\n",
      "\tClass Weights: None\n",
      "==================== Random Forest Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.72      0.98      0.83       964\n",
      "Scottish National Party       0.76      0.45      0.56       463\n",
      "                 Labour       0.00      0.00      0.00        54\n",
      "       Liberal Democrat       0.87      0.25      0.39       136\n",
      "\n",
      "               accuracy                           0.73      1617\n",
      "              macro avg       0.59      0.42      0.45      1617\n",
      "           weighted avg       0.72      0.73      0.69      1617\n",
      "\n",
      "==================== SVC Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.85      0.92      0.88       964\n",
      "Scottish National Party       0.76      0.73      0.74       463\n",
      "                 Labour       1.00      0.22      0.36        54\n",
      "       Liberal Democrat       0.72      0.57      0.64       136\n",
      "\n",
      "               accuracy                           0.81      1617\n",
      "              macro avg       0.83      0.61      0.66      1617\n",
      "           weighted avg       0.81      0.81      0.80      1617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "section_c = ml_pipeline(data = df_cleaned, stop_words = 'english')\n",
    "# Save results into a dictionary\n",
    "f1_results['f1_ma_rf_unigram'] =  round(section_c['rf']['macro avg']['f1-score'] ,2)\n",
    "f1_results['f1_ma_svc_unigram'] = round(section_c['svc']['macro avg']['f1-score'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "deea5cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arguments:\n",
      "\tNgram: (1, 1)\n",
      "\tStop words: english\n",
      "\tTokenizer: None\n",
      "\tClass Weights: balanced\n",
      "==================== Random Forest Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.70      0.98      0.82       964\n",
      "Scottish National Party       0.83      0.36      0.50       463\n",
      "                 Labour       1.00      0.02      0.04        54\n",
      "       Liberal Democrat       0.86      0.41      0.56       136\n",
      "\n",
      "               accuracy                           0.72      1617\n",
      "              macro avg       0.85      0.44      0.48      1617\n",
      "           weighted avg       0.76      0.72      0.68      1617\n",
      "\n",
      "==================== SVC Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.87      0.88      0.88       964\n",
      "Scottish National Party       0.74      0.74      0.74       463\n",
      "                 Labour       0.66      0.39      0.49        54\n",
      "       Liberal Democrat       0.65      0.70      0.67       136\n",
      "\n",
      "               accuracy                           0.81      1617\n",
      "              macro avg       0.73      0.68      0.70      1617\n",
      "           weighted avg       0.81      0.81      0.81      1617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "section_c_balanced = ml_pipeline(data = df_cleaned, stop_words = 'english', class_weight = 'balanced')\n",
    "# Save results into a dictionary\n",
    "f1_results['f1_ma_rf_unigram_balanced'] =  round(section_c_balanced['rf']['macro avg']['f1-score'] ,2)\n",
    "f1_results['f1_ma_svc_unigram_balanced'] = round(section_c_balanced['svc']['macro avg']['f1-score'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f7cf60",
   "metadata": {},
   "source": [
    "#### Model Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b9eba",
   "metadata": {},
   "source": [
    "Train a Random Forest Model and SVM linear Kernel model:\n",
    "\n",
    "    Remove English stop word: Yes\n",
    "    Ngram: unigram, bi-gram and tri-grams\n",
    "    Tokenizer: Default\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8f6a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arguments:\n",
      "\tNgram: (1, 3)\n",
      "\tStop words: english\n",
      "\tTokenizer: None\n",
      "\tClass Weights: None\n",
      "==================== Random Forest Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.74      0.97      0.84       964\n",
      "Scottish National Party       0.78      0.49      0.60       463\n",
      "                 Labour       0.00      0.00      0.00        54\n",
      "       Liberal Democrat       0.82      0.36      0.50       136\n",
      "\n",
      "               accuracy                           0.75      1617\n",
      "              macro avg       0.58      0.45      0.48      1617\n",
      "           weighted avg       0.73      0.75      0.71      1617\n",
      "\n",
      "==================== SVC Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.85      0.92      0.88       964\n",
      "Scottish National Party       0.75      0.73      0.74       463\n",
      "                 Labour       0.90      0.17      0.28        54\n",
      "       Liberal Democrat       0.80      0.63      0.70       136\n",
      "\n",
      "               accuracy                           0.82      1617\n",
      "              macro avg       0.82      0.61      0.65      1617\n",
      "           weighted avg       0.82      0.82      0.81      1617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "section_d = ml_pipeline(data = df_cleaned, ngram = (1,3), stop_words = 'english')\n",
    "# Save results into a dictionary\n",
    "f1_results['f1_ma_rf_uni_bi_trigrams'] =  round(section_d['rf']['macro avg']['f1-score'] ,2)\n",
    "f1_results['f1_ma_svc_uni_bi_trigrams'] = round(section_d['svc']['macro avg']['f1-score'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7f2ea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arguments:\n",
      "\tNgram: (1, 3)\n",
      "\tStop words: english\n",
      "\tTokenizer: None\n",
      "\tClass Weights: balanced\n",
      "==================== Random Forest Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.71      0.96      0.82       964\n",
      "Scottish National Party       0.80      0.40      0.53       463\n",
      "                 Labour       0.00      0.00      0.00        54\n",
      "       Liberal Democrat       0.83      0.49      0.62       136\n",
      "\n",
      "               accuracy                           0.73      1617\n",
      "              macro avg       0.58      0.46      0.49      1617\n",
      "           weighted avg       0.72      0.73      0.69      1617\n",
      "\n",
      "==================== SVC Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.87      0.90      0.89       964\n",
      "Scottish National Party       0.75      0.74      0.75       463\n",
      "                 Labour       0.53      0.37      0.43        54\n",
      "       Liberal Democrat       0.70      0.69      0.70       136\n",
      "\n",
      "               accuracy                           0.82      1617\n",
      "              macro avg       0.71      0.67      0.69      1617\n",
      "           weighted avg       0.81      0.82      0.82      1617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "section_d_balanced = ml_pipeline(data = df_cleaned, ngram = (1,3), stop_words = 'english', class_weight = 'balanced')\n",
    "# Save results into a dictionary\n",
    "f1_results['f1_ma_rf_uni_bi_trigrams_balanced'] =  round(section_d_balanced['rf']['macro avg']['f1-score'] ,2)\n",
    "f1_results['f1_ma_svc_uni_bi_trigrams_balanced'] = round(section_d_balanced['svc']['macro avg']['f1-score'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782cab7",
   "metadata": {},
   "source": [
    "### Custom Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508aafcb",
   "metadata": {},
   "source": [
    "**Basic Tokenizer**\n",
    "\n",
    "`my_tokenizer_basic`\n",
    "\n",
    "- Removes special break characters, such as \\n, \\t etc\n",
    "- Removes any extra white spaces \n",
    "- Uses `nltk` word tokenizer to split the words into objects\n",
    "- Only keeps alphabetical objects, ignores numeric and punctuation marks\n",
    "- It keeps English stop words\n",
    "\n",
    "From the metrics below, we can see that this tokenizer did not improve performance on the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "538f5c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arguments:\n",
      "\tNgram: (1, 3)\n",
      "\tStop words: None\n",
      "\tTokenizer: my_tokenizer_basic\n",
      "\tClass Weights: balanced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Random Forest Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.71      0.98      0.82       964\n",
      "Scottish National Party       0.84      0.37      0.51       463\n",
      "                 Labour       0.00      0.00      0.00        54\n",
      "       Liberal Democrat       0.82      0.46      0.59       136\n",
      "\n",
      "               accuracy                           0.73      1617\n",
      "              macro avg       0.59      0.45      0.48      1617\n",
      "           weighted avg       0.73      0.73      0.68      1617\n",
      "\n",
      "==================== SVC Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.88      0.88      0.88       964\n",
      "Scottish National Party       0.73      0.74      0.73       463\n",
      "                 Labour       0.50      0.33      0.40        54\n",
      "       Liberal Democrat       0.68      0.72      0.70       136\n",
      "\n",
      "               accuracy                           0.81      1617\n",
      "              macro avg       0.70      0.67      0.68      1617\n",
      "           weighted avg       0.81      0.81      0.81      1617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "section_e_basic_t =  ml_pipeline(data = df_cleaned, ngram = (1,3), tokenizer = my_tokenizer_basic, class_weight='balanced')\n",
    "f1_results['f1_ma_rf_uni_bi_trigrams_basictoken'] =  round(section_e_basic_t['rf']['macro avg']['f1-score'] ,2)\n",
    "f1_results['f1_ma_svc_uni_bi_trigrams_basictoken'] = round(section_e_basic_t['svc']['macro avg']['f1-score'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1da2c",
   "metadata": {},
   "source": [
    "**Sentence Tokenizer**\n",
    "\n",
    "Using a sentence tokenizer produced worse results than previous tokenizers. A potential reason is that splitting the text by sentences instead of words generates a lower proportion of unique values. There will be less repeated sentences than repeated words, therefore this reduces the dimensionality and data availability for the model to actually learn from the data, given that with less repetition of token_x, label_x pairs, it becomes more difficult to the model to generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb65d46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arguments:\n",
      "\tNgram: (1, 1)\n",
      "\tStop words: None\n",
      "\tTokenizer: my_tokenizer_sentence\n",
      "\tClass Weights: balanced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Random Forest Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.75      0.43      0.54       964\n",
      "Scottish National Party       0.34      0.67      0.45       463\n",
      "                 Labour       0.04      0.06      0.05        54\n",
      "       Liberal Democrat       0.24      0.17      0.20       136\n",
      "\n",
      "               accuracy                           0.46      1617\n",
      "              macro avg       0.34      0.33      0.31      1617\n",
      "           weighted avg       0.57      0.46      0.47      1617\n",
      "\n",
      "==================== SVC Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.63      0.88      0.74       964\n",
      "Scottish National Party       0.42      0.12      0.19       463\n",
      "                 Labour       0.07      0.07      0.07        54\n",
      "       Liberal Democrat       0.29      0.16      0.21       136\n",
      "\n",
      "               accuracy                           0.58      1617\n",
      "              macro avg       0.35      0.31      0.30      1617\n",
      "           weighted avg       0.52      0.58      0.51      1617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "section_e_sentence_t =  ml_pipeline(data = df_cleaned, ngram = (1,1), tokenizer = my_tokenizer_sentence, class_weight='balanced')\n",
    "f1_results['f1_ma_rf_uni_bi_trigrams_sentencetoken'] =  round(section_e_sentence_t['rf']['macro avg']['f1-score'] ,2)\n",
    "f1_results['f1_ma_svc_uni_bi_trigrams_sentencetoken'] = round(section_e_sentence_t['svc']['macro avg']['f1-score'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573e19f",
   "metadata": {},
   "source": [
    "**Spacy/Contractions Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d69afbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arguments:\n",
      "\tNgram: (1, 3)\n",
      "\tStop words: None\n",
      "\tTokenizer: my_tokenizer_spacy\n",
      "\tClass Weights: balanced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Random Forest Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.70      0.98      0.81       964\n",
      "Scottish National Party       0.82      0.34      0.48       463\n",
      "                 Labour       0.00      0.00      0.00        54\n",
      "       Liberal Democrat       0.81      0.41      0.55       136\n",
      "\n",
      "               accuracy                           0.72      1617\n",
      "              macro avg       0.58      0.43      0.46      1617\n",
      "           weighted avg       0.72      0.72      0.67      1617\n",
      "\n",
      "==================== SVC Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.88      0.88      0.88       964\n",
      "Scottish National Party       0.73      0.73      0.73       463\n",
      "                 Labour       0.50      0.33      0.40        54\n",
      "       Liberal Democrat       0.67      0.72      0.70       136\n",
      "\n",
      "               accuracy                           0.81      1617\n",
      "              macro avg       0.69      0.67      0.68      1617\n",
      "           weighted avg       0.81      0.81      0.81      1617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "section_e_spacy_t =  ml_pipeline(data = df_cleaned, ngram = (1,3), tokenizer = my_tokenizer_spacy, class_weight='balanced')\n",
    "f1_results['f1_ma_rf_uni_bi_trigrams_spacytoken'] =  round(section_e_spacy_t['rf']['macro avg']['f1-score'] ,2)\n",
    "f1_results['f1_ma_svc_uni_bi_trigrams_spacytoken'] = round(section_e_spacy_t['svc']['macro avg']['f1-score'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1159c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arguments:\n",
      "\tNgram: (1, 1)\n",
      "\tStop words: None\n",
      "\tTokenizer: my_tokenizer_contractions_clean\n",
      "\tClass Weights: balanced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Random Forest Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.68      0.99      0.80       964\n",
      "Scottish National Party       0.88      0.29      0.43       463\n",
      "                 Labour       0.00      0.00      0.00        54\n",
      "       Liberal Democrat       0.88      0.31      0.46       136\n",
      "\n",
      "               accuracy                           0.70      1617\n",
      "              macro avg       0.61      0.40      0.42      1617\n",
      "           weighted avg       0.73      0.70      0.64      1617\n",
      "\n",
      "==================== SVC Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.88      0.90      0.89       964\n",
      "Scottish National Party       0.77      0.73      0.75       463\n",
      "                 Labour       0.57      0.44      0.50        54\n",
      "       Liberal Democrat       0.64      0.71      0.67       136\n",
      "\n",
      "               accuracy                           0.82      1617\n",
      "              macro avg       0.71      0.70      0.70      1617\n",
      "           weighted avg       0.82      0.82      0.82      1617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "section_e_contractcleaned_t =  ml_pipeline(data = df_cleaned, ngram = (1,1), tokenizer = my_tokenizer_contractions_clean, class_weight='balanced')#, stop_words = 'english')\n",
    "f1_results['f1_ma_rf_uni_bi_trigrams_contractcleantoken'] =  round(section_e_contractcleaned_t['rf']['macro avg']['f1-score'] ,2)\n",
    "f1_results['f1_ma_svc_uni_bi_trigrams_contractcleantoken'] = round(section_e_contractcleaned_t['svc']['macro avg']['f1-score'], 2)\n",
    "\n",
    "#)1,3) stop words removed: svc.56\n",
    "#(1,2) stop words removed: svc.70\n",
    "#(1,1) stop words removed: svc.70\n",
    "\n",
    "#(1,3) no stop words removed: svc.68\n",
    "#(1,2) no stop words removed: svc.67\n",
    "#(1,1) no stop words removed: svc.70\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ea2cfea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arguments:\n",
      "\tNgram: (1, 3)\n",
      "\tStop words: english\n",
      "\tTokenizer: my_tokenizer_lemma\n",
      "\tClass Weights: balanced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Random Forest Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.71      0.97      0.82       964\n",
      "Scottish National Party       0.82      0.38      0.52       463\n",
      "                 Labour       0.00      0.00      0.00        54\n",
      "       Liberal Democrat       0.86      0.48      0.61       136\n",
      "\n",
      "               accuracy                           0.73      1617\n",
      "              macro avg       0.59      0.46      0.49      1617\n",
      "           weighted avg       0.73      0.73      0.69      1617\n",
      "\n",
      "==================== SVC Performance ====================\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Conservative       0.87      0.90      0.89       964\n",
      "Scottish National Party       0.77      0.74      0.75       463\n",
      "                 Labour       0.53      0.44      0.48        54\n",
      "       Liberal Democrat       0.71      0.71      0.71       136\n",
      "\n",
      "               accuracy                           0.82      1617\n",
      "              macro avg       0.72      0.70      0.71      1617\n",
      "           weighted avg       0.82      0.82      0.82      1617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/enmanuelmoreno/.local/share/virtualenvs/nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "section_e_lemma_t =  ml_pipeline(data = df_cleaned, ngram = (1,3), tokenizer = my_tokenizer_lemma, class_weight='balanced', stop_words = 'english')\n",
    "f1_results['f1_ma_rf_uni_bi_trigrams_lemmatoken'] =  round(section_e_lemma_t['rf']['macro avg']['f1-score'] ,2)\n",
    "f1_results['f1_ma_svc_uni_bi_trigrams_lemmatoken'] = round(section_e_lemma_t['svc']['macro avg']['f1-score'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74fc4c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Avg Score f1_ma_rf_unigram                                   Value:   0.45\n",
      "F1 Macro Avg Score f1_ma_svc_unigram                                  Value:   0.66\n",
      "F1 Macro Avg Score f1_ma_rf_unigram_balanced                          Value:   0.48\n",
      "F1 Macro Avg Score f1_ma_svc_unigram_balanced                         Value:   0.70\n",
      "F1 Macro Avg Score f1_ma_rf_uni_bi_trigrams                           Value:   0.48\n",
      "F1 Macro Avg Score f1_ma_svc_uni_bi_trigrams                          Value:   0.65\n",
      "F1 Macro Avg Score f1_ma_rf_uni_bi_trigrams_balanced                  Value:   0.49\n",
      "F1 Macro Avg Score f1_ma_svc_uni_bi_trigrams_balanced                 Value:   0.69\n",
      "F1 Macro Avg Score f1_ma_rf_uni_bi_trigrams_basictoken                Value:   0.48\n",
      "F1 Macro Avg Score f1_ma_svc_uni_bi_trigrams_basictoken               Value:   0.68\n",
      "F1 Macro Avg Score f1_ma_rf_uni_bi_trigrams_sentencetoken             Value:   0.31\n",
      "F1 Macro Avg Score f1_ma_svc_uni_bi_trigrams_sentencetoken            Value:   0.30\n",
      "F1 Macro Avg Score f1_ma_rf_uni_bi_trigrams_spacytoken                Value:   0.46\n",
      "F1 Macro Avg Score f1_ma_svc_uni_bi_trigrams_spacytoken               Value:   0.68\n",
      "F1 Macro Avg Score f1_ma_rf_uni_bi_trigrams_contractcleantoken        Value:   0.41\n",
      "F1 Macro Avg Score f1_ma_svc_uni_bi_trigrams_contractcleantoken       Value:   0.70\n",
      "F1 Macro Avg Score f1_ma_rf_uni_bi_trigrams_lemmatoken                Value:   0.49\n",
      "F1 Macro Avg Score f1_ma_svc_uni_bi_trigrams_lemmatoken               Value:   0.71\n",
      "F1 Macro Avg Score f1_ma_rf_uni_bi_trigrams_treebanktoken             Value:   0.48\n",
      "F1 Macro Avg Score f1_ma_svc_uni_bi_trigrams_treebanktoken            Value:   0.68\n"
     ]
    }
   ],
   "source": [
    "for key, value in f1_results.items():\n",
    "    print(f\"F1 Macro Avg Score {key:<50} Value: {value:6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaaf746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-coursework-2024-25-enmanuelmorego-pEh8u7DC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
